This course drew from the following resources:

- Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer (Raffel et al, 2019)
> https://arxiv.org/abs/1910.10683

- Reformer: The Efficient Transformer (Kitaev et al, 2020)
> https://arxiv.org/abs/2001.04451


- Attention Is All You Need (Vaswani et al, 2017)
> https://arxiv.org/abs/1706.03762

- Deep contextualized word representations (Peters et al, 2018)
> https://arxiv.org/pdf/1802.05365.pdf

- The Illustrated Transformer (Alammar, 2018)
> http://jalammar.github.io/illustrated-transformer/

- The Illustrated GPT-2 (Visualizing Transformer Language Models) (Alammar, 2019)
> http://jalammar.github.io/illustrated-gpt2/

- BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (Devlin et al, 2018)
> https://arxiv.org/abs/1810.04805

- How GPT3 Works - Visualizations and Animations (Alammar, 2020)
> http://jalammar.github.io/how-gpt3-works-visualizations-animations/